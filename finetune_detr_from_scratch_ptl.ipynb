{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "372f2d44",
   "metadata": {},
   "source": [
    "In my last notebook, I finetuned using a fork of DETR (on the balloons dataset) made by another github user. Most of my work was just copy and pasted from them. Additionally, that approach used a command line invocation of the `main.py` file that facebook originally used to train the model. This is quite heavy and inflexible as the fine tuning code had to use the research code. It would be much better from a learning and usability standpoint if I could move away from the DETR research repo and instead write the fine tuning process myself. I am going to try to finetune DETR \"from scratch\". Ideally, I try to meet or exceed the performance achieved using the research code. Additionally, I am going to take this opportunity to use pytorch lightning instead of plain old pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f89b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
